{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "541178f4-5120-4d3a-8c2c-3aebca0ca94b",
   "metadata": {},
   "source": [
    "# QM/MM in OpenMM\n",
    "\n",
    "One side effect of interfacing EMLE with OpenMM through Sire is that is now possible to perform 'normal' QM/MM using OpenMM (which has not been previously available). This includes the same 'black-box' implementation for hydrogen link-atoms across covalent bonds between the QM and MM region as available in AMBER (sander). \n",
    "\n",
    "Note that when using the EMLEcalculator interface, such QM/MM calculations still use EMLE for the electrostatic embedding (instead of including the MM partial charges directly in the QM calculations, as is performed in standard QM/MM). To run 'standard' QM/MM with the XTB code, a full xtb QM engine for use with Sire should be created, which can be done following the approach shown [here](https://sire.openbiosim.org/tutorial/part08/01_intro.html#creating-a-qm-engine).\n",
    "\n",
    "In this tutorial, we will first go through performing a QM/MM simulation with XTB2 (GFN2-xTB), using EMLE-embedding, and then perform the equivalent simulation with a MACE MLP. This will be done for the same AbyU system as before. However, in these examples, we also will show that the ML region does not need to be a whole molecule. This is important for most enzyme reactions, where amino-acid side chains also take part. \n",
    "\n",
    "In this instance, the QM (or ML) region will be the same AbyU substrate as before, but we will now also include the side-chain of a tryptophan residue that is next to the substrate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cad0431-dc3b-4092-b73a-c462e2cb47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import openmm\n",
    "import openmm.unit as unit\n",
    "from openmm.app import *\n",
    "from openmm import CustomBondForce, CustomCVForce\n",
    "\n",
    "from emle.models import MACEEMLE\n",
    "from emle.calculator import EMLECalculator\n",
    "\n",
    "import sire as sr\n",
    "\n",
    "distances = ((2125, 2094, 0.7), (2119, 2087, 0.3))\n",
    "k = (125*unit.kilocalorie_per_mole/unit.angstrom**2).value_in_unit(unit.kilojoule_per_mole/unit.nanometer**2)\n",
    "r0 = 2.5*unit.angstroms\n",
    "\n",
    "cv = CustomBondForce(\"weight*r\") \n",
    "cv.addPerBondParameter(\"weight\") \n",
    "\n",
    "for atom1, atom2, weight in distances: \n",
    "    cv.addBond(atom1, atom2, [weight]) \n",
    "\n",
    "energy_expression = \"k*(weighted_distance - r0)^2\"\n",
    "restraint_force = openmm.CustomCVForce(energy_expression)\n",
    "restraint_force.addCollectiveVariable(\"weighted_distance\", cv)\n",
    "restraint_force.addGlobalParameter(\"k\", k)\n",
    "restraint_force.addGlobalParameter(\"r0\", r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdd7c2-5bba-4b11-adad-0b24d3750b4e",
   "metadata": {},
   "source": [
    "# Creating the system\n",
    "\n",
    "Again, this is similar to what you have seen before. XTB2 is a supported backend for EMLE already (though you can also use ORCA as a backend, or custom MLPs so long as they support ASE; further details can be found in the [EMLE documentation](https://chemle.github.io/emle-engine/)).\n",
    "\n",
    "As the QM region now incorporates the tryptophan side-chain, the atom selection must change. Whilst before, we could simply select the seperate substrate molecule, now we must list the actual atoms involved (here as ranges of atom indices) as they are across two different molecules (the enzyme and the substrate). \n",
    "\n",
    "For the creation of an OpenMM context and the production run, there are no changes from before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a99f782-cf19-446e-a14a-e8f73ca98e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a3416caa1049c6b196195450a9f274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/emle/models/_mace.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  source_model = _torch.load(mace_model, map_location=device)\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/mace/modules/models.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Loads AbyU system\n",
    "mols = sr.load([f\"AbyU_OpenMM.rst\", f\"AbyU_OpenMM.prm7\"])\n",
    "\n",
    "# Load the topology with OpenMM too.\n",
    "prm = AmberPrmtopFile(f\"AbyU_OpenMM.prm7\")\n",
    "\n",
    "#Create calculator\n",
    "#calculator = EMLECalculator(device=\"cpu\", backend=\"xtb\")\n",
    "calculator = EMLECalculator(device=\"cpu\", mace_model=\"models/abyu_mace_link.model\", backend=\"mace\")\n",
    "\n",
    "# Create an EMLEEngine bound to the calculator on the substrate.\n",
    "mols, engine = sr.qm.emle(mols, \"atomnum 1804:1822,2083:2132\", calculator, redistribute_charge=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70501592-55d8-404a-a9b1-57150100941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QM/MM dynamics object.\n",
    "d = mols.dynamics(\n",
    "    timestep=\"1fs\",\n",
    "    constraint=\"none\",\n",
    "    integrator=\"langevin_middle\",\n",
    "    temperature=\"300K\",\n",
    "    qm_engine=engine,\n",
    "    map={\"threads\": 1},\n",
    "    fixed=\"not atoms within 20A of atomidx 3 in mol[1]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360ff3db-bae1-4149-97b8-44c202e119f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the underlying OpenMM context.\n",
    "context = d._d._omm_mols\n",
    "\n",
    "# Get the OpenMM system.\n",
    "omm_system = context.getSystem()\n",
    "\n",
    "# Store a copy of the integrator.\n",
    "integrator = context.getIntegrator().__copy__()\n",
    "\n",
    "# Add the forces to the OpenMM system.\n",
    "omm_system.addForce(restraint_force)\n",
    "\n",
    "# Create a new context.\n",
    "new_context = openmm.Context(omm_system, integrator, context.getPlatform())\n",
    "\n",
    "# Set force constant K for the biasing potential.\n",
    "new_context.setParameter(\"k\", k)\n",
    "\n",
    "#Set center of biasing potential\n",
    "new_context.setParameter(\"r0\", r0)\n",
    "\n",
    "# Set the postions of the new context to be the same as the original context.\n",
    "new_context.setPositions(context.getState(getPositions=True).getPositions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6780038-a686-4c78-91db-5825e641d9e1",
   "metadata": {},
   "source": [
    "# Running the simulation\n",
    "Here we've saved the trajectory file as \"AbyU_OpenMM_xtb.dcd\". \n",
    "\n",
    "For loading the trajectory in NGLviewer, we have also added in the tryptophan side chain (you can see that the atom listing is the same as when we define the \"QM\" or \"ML\" region.). \n",
    "\n",
    "Once you've run the XTB simulation, we suggest that you edit code to swap calculators to the MACE MLP in the system setup, and then you can rerun this simulation using ML/MM. (In this instance the MLP was trained to the XTB2 level for ease of training, but you should see that they produce very similar results). \n",
    "\n",
    "For viewing the resulting trajectory, you should change the file name in the \"file_handle\" and trajectory loading below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735c0037-262b-483b-b726-ae7176880e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/nn/modules/module.py:1562: UserWarning: at::frobenius_norm is deprecated and it is just left for JIT compatibility. It will be removed in a future PyTorch release. Please use `linalg.vector_norm(A, 2., dim, keepdim)` instead (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1729124132187/work/aten/src/ATen/native/LinearAlgebra.cpp:3072.)\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Sampling production. Trajectories are saved in dcd files.\n",
    "file_handle = open(f\"./AbyU_OpenMM/AbyU_OpenMM_mace.dcd\", \"bw\")\n",
    "state = new_context.getState(getPositions=True)\n",
    "positions = state.getPositions()\n",
    "dcd_file = DCDFile(file_handle, prm.topology, dt=integrator.getStepSize())\n",
    "dcd_file.writeModel(positions)\n",
    "\n",
    "for x in range(300):\n",
    "    integrator.step(1)\n",
    "    state = new_context.getState(getPositions=True)\n",
    "    positions = state.getPositions()\n",
    "    dcd_file.writeModel(positions)\n",
    "        \n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acf0b95-08f3-4b79-a1f2-692e1695492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0400d7511b402bbb46151bd28e0e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(max_frame=300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traj = sr.load(\"AbyU_OpenMM.prm7\", \"AbyU_OpenMM/AbyU_OpenMM_mace.dcd\")\n",
    "\n",
    "#To view the whole system\n",
    "#traj.trajectory().view()\n",
    "\n",
    "# To view just the substrate\n",
    "traj.atoms()[\"atomnum 1804:1822,2083:2132\"].trajectory().view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmm-workshop2",
   "language": "python",
   "name": "mlmm-workshop2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
