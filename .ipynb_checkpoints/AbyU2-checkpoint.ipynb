{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "541178f4-5120-4d3a-8c2c-3aebca0ca94b",
   "metadata": {},
   "source": [
    "# QM/MM in OpenMM\n",
    "\n",
    "One side effect of interfacing EMLE with OpenMM through Sire is we can now also perform normal QM/MM using OpenMM (which has not been previously available). In this tutorial we will go through performing a QM/MM simulation with XTB2, and then perform the equivalent with a MACE MLP of this same system. \n",
    "\n",
    "We also will show that you are not constrained to having the ML region be a whole molecule. In this instance, the QM (or ML) region will be the same subtrate as before, but will now also include the side chain of a tryptophan that is next to the substrate in enzyme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cad0431-dc3b-4092-b73a-c462e2cb47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import openmm\n",
    "import openmm.unit as unit\n",
    "from openmm.app import *\n",
    "from openmm import CustomBondForce, CustomCVForce\n",
    "\n",
    "from emle.models import MACEEMLE\n",
    "from emle.calculator import EMLECalculator\n",
    "\n",
    "import sire as sr\n",
    "\n",
    "distances = ((2125, 2094, 0.7), (2119, 2087, 0.3))\n",
    "k = (75*unit.kilocalorie_per_mole/unit.angstrom**2).value_in_unit(unit.kilojoule_per_mole/unit.nanometer**2)\n",
    "r0 = 2.5*unit.angstroms\n",
    "\n",
    "cv = CustomBondForce(\"weight*r\") \n",
    "cv.addPerBondParameter(\"weight\") \n",
    "\n",
    "for atom1, atom2, weight in distances: \n",
    "    cv.addBond(atom1, atom2, [weight]) \n",
    "\n",
    "energy_expression = \"k*(weighted_distance - r0)^2\"\n",
    "restraint_force = openmm.CustomCVForce(energy_expression)\n",
    "restraint_force.addCollectiveVariable(\"weighted_distance\", cv)\n",
    "restraint_force.addGlobalParameter(\"k\", k)\n",
    "restraint_force.addGlobalParameter(\"r0\", r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdd7c2-5bba-4b11-adad-0b24d3750b4e",
   "metadata": {},
   "source": [
    "# Creating the system\n",
    "\n",
    "Again this is similar to what you have seen before. XTB2 is a supported backend for EMLE already (though you can use ORCA as a backend, or custom MLPs so long as they support ASE, further details of which can be found in the documentation).\n",
    "\n",
    "As the QM region now incorporates the tryptophan side chain, the atom selection must change. Whilst before, as we used a seperated molecule we could just choose it, now we must list the actual atoms involved as they cross two different molecules (the enzyme and the substrate). \n",
    "\n",
    "For the actual creation of an OpenMM context and the production run, there are no changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a99f782-cf19-446e-a14a-e8f73ca98e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380b36939daf4f1e822db176326b5e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/emle/models/_mace.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  source_model = _torch.load(mace_model, map_location=device)\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/mace/modules/models.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Loads AbyU system\n",
    "mols = sr.load([f\"out.rst\", f\"AbyU_OpenMM.prm7\"])\n",
    "\n",
    "# Load the topology with OpenMM too.\n",
    "prm = AmberPrmtopFile(f\"AbyU_OpenMM.prm7\")\n",
    "\n",
    "#Create calculator\n",
    "#calculator = EMLECalculator(device=\"cpu\", backend=\"xtb\")\n",
    "calculator = EMLECalculator(device=\"cpu\", mace_model=\"abyu_mace_link.model\", backend=\"mace\")\n",
    "\n",
    "# Create an EMLEEngine bound to the calculator on the substrate.\n",
    "mols, engine = sr.qm.emle(mols, \"atomnum 1804:1822,2083:2132\", calculator, redistribute_charge=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70501592-55d8-404a-a9b1-57150100941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QM/MM dynamics object.\n",
    "d = mols.dynamics(\n",
    "    timestep=\"1fs\",\n",
    "    constraint=\"none\",\n",
    "    integrator=\"langevin_middle\",\n",
    "    temperature=\"300K\",\n",
    "    qm_engine=engine,\n",
    "    map={\"threads\": 1},\n",
    "    fixed=\"not atoms within 20A of atomidx 3 in mol[1]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360ff3db-bae1-4149-97b8-44c202e119f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the underlying OpenMM context.\n",
    "context = d._d._omm_mols\n",
    "\n",
    "# Get the OpenMM system.\n",
    "omm_system = context.getSystem()\n",
    "\n",
    "# Store a copy of the integrator.\n",
    "integrator = context.getIntegrator().__copy__()\n",
    "\n",
    "# Add the forces to the OpenMM system.\n",
    "omm_system.addForce(restraint_force)\n",
    "\n",
    "# Create a new context.\n",
    "new_context = openmm.Context(omm_system, integrator, context.getPlatform())\n",
    "\n",
    "# Set force constant K for the biasing potential.\n",
    "new_context.setParameter(\"k\", k)\n",
    "\n",
    "#Set center of biasing potential\n",
    "new_context.setParameter(\"r0\", r0)\n",
    "\n",
    "# Set the postions of the new context to be the same as the original context.\n",
    "new_context.setPositions(context.getState(getPositions=True).getPositions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6780038-a686-4c78-91db-5825e641d9e1",
   "metadata": {},
   "source": [
    "# Running the simulation\n",
    "Here we've saved the trajectory file as \"AbyU_OpenMM_xtb.dcd\". \n",
    "\n",
    "For loading the trajectory in NGLviewer, we have also added in the tryptophan side chain (you can see the atom listing is the same as when we define the QM/ML region.). \n",
    "\n",
    "Once you've run the XTB simulation, edit the file name in the \"file_handle\" and trajectory loading and swap calculators to the MACE MLP earlier when we create the system and you can rerun this system using ML/MM (in this instance the MLP was trained to the XTB2 level for ease of training, but you should see that they produce very similar results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735c0037-262b-483b-b726-ae7176880e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/nn/modules/module.py:1562: UserWarning: at::frobenius_norm is deprecated and it is just left for JIT compatibility. It will be removed in a future PyTorch release. Please use `linalg.vector_norm(A, 2., dim, keepdim)` instead (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1729124132187/work/aten/src/ATen/native/LinearAlgebra.cpp:3072.)\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Sampling production. Trajectories are saved in dcd files.\n",
    "file_handle = open(f\"./AbyU_OpenMM/AbyU_OpenMM_mace.dcd\", \"bw\")\n",
    "state = new_context.getState(getPositions=True)\n",
    "positions = state.getPositions()\n",
    "dcd_file = DCDFile(file_handle, prm.topology, dt=integrator.getStepSize())\n",
    "dcd_file.writeModel(positions)\n",
    "\n",
    "for x in range(100):\n",
    "    integrator.step(10)\n",
    "    state = new_context.getState(getPositions=True)\n",
    "    positions = state.getPositions()\n",
    "    dcd_file.writeModel(positions)\n",
    "        \n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acf0b95-08f3-4b79-a1f2-692e1695492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68f6703170a4aa3b6518916548d6289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(max_frame=30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traj = sr.load(\"AbyU_OpenMM.prm7\", \"AbyU_OpenMM/AbyU_OpenMM_mace.dcd\")\n",
    "\n",
    "#To view the whole system\n",
    "#traj.trajectory().view()\n",
    "\n",
    "# To view just the substrate\n",
    "traj.atoms()[\"atomnum 1804:1822,2083:2132\"].trajectory().view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmm-workshop2",
   "language": "python",
   "name": "mlmm-workshop2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
