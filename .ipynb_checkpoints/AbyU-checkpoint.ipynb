{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffe7633-4800-4533-b698-9dda5a93dfb9",
   "metadata": {},
   "source": [
    "# AbyU OpenMM\n",
    "\n",
    "![AbyU!](AbyU.png)\n",
    "\n",
    "Now we will perform an ML/MM reaction simulation, specifically of the reaction catalysed by AbyU.\n",
    "\n",
    "In this instance, we have a pretrained MACE MLP (trained to the M06-2X/6-31G* level) and use the generic embedding model. To perform this reaction we use a generalised distance restraint between two bonds, so initially we define a restraint on these bonds and add it as a custom bond in OpenMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000ddd7d-f74d-4b39-83be-3ae92258f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import openmm\n",
    "import openmm.unit as unit\n",
    "from openmm.app import *\n",
    "from openmm import CustomBondForce, CustomCVForce\n",
    "\n",
    "from emle.models import MACEEMLE\n",
    "from emle.calculator import EMLECalculator\n",
    "\n",
    "import sire as sr\n",
    "\n",
    "distances = ((2125, 2094, 0.7), (2119, 2087, 0.3))\n",
    "k = (75*unit.kilocalorie_per_mole/unit.angstrom**2).value_in_unit(unit.kilojoule_per_mole/unit.nanometer**2)\n",
    "r0 = 2.5*unit.angstroms\n",
    "\n",
    "cv = CustomBondForce(\"weight*r\") \n",
    "cv.addPerBondParameter(\"weight\") \n",
    "\n",
    "for atom1, atom2, weight in distances: \n",
    "    cv.addBond(atom1, atom2, [weight]) \n",
    "\n",
    "energy_expression = \"k*(weighted_distance - r0)^2\"\n",
    "restraint_force = openmm.CustomCVForce(energy_expression)\n",
    "restraint_force.addCollectiveVariable(\"weighted_distance\", cv)\n",
    "restraint_force.addGlobalParameter(\"k\", k)\n",
    "restraint_force.addGlobalParameter(\"r0\", r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdd7c2-5bba-4b11-adad-0b24d3750b4e",
   "metadata": {},
   "source": [
    "# Creating the system\n",
    "\n",
    "Much like the ADP system, we initially load the system in Sire and OpenMM. The main difference here is in the creation of the calculator, as we're using a MACE MLP instead of the default ANI-2x so we tell the calculator to load a different backend and give it the model file (which you can see in the lift of files on the left). (You will see warnings as the calculator loads but that is okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a99f782-cf19-446e-a14a-e8f73ca98e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6e078c6ea64e319f2d614687c676c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/emle/models/_mace.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  source_model = _torch.load(mace_model, map_location=device)\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/mace/modules/models.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Loads AbyU system\n",
    "mols = sr.load([f\"AbyU_OpenMM.rst\", f\"AbyU_OpenMM.prm7\"])\n",
    "\n",
    "# Load the topology with OpenMM too.\n",
    "prm = AmberPrmtopFile(f\"AbyU_OpenMM.prm7\")\n",
    "\n",
    "#Create calculator\n",
    "calculator = EMLECalculator(device=\"cpu\", mace_model=\"abyu_mace.model\", backend=\"mace\")\n",
    "\n",
    "# Create an EMLEEngine bound to the calculator on the substrate.\n",
    "mols, engine = sr.qm.emle(mols, mols[1], calculator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5168c-baa8-4a3a-8ab1-e8e8b04eabcd",
   "metadata": {},
   "source": [
    "# Running the simulation\n",
    "\n",
    "This is again similar to what you previously saw in the ADP example, but here we have to add a few extra settings. We have a couple new settings for the dynamics object, most obviously is the fixed atoms. This settings emulates the iBelly restraints found in AMBER, fixing all atoms outside of a 20A radius of the substrate in place. This is done for simulation efficiency. \n",
    "\n",
    "We also add some lines in the OpenMM context, specifically to add the biasing potentials to the system. The actual production cell runs the same as in the ADP example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70501592-55d8-404a-a9b1-57150100941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QM/MM dynamics object.\n",
    "d = mols.dynamics(\n",
    "    timestep=\"1fs\",\n",
    "    constraint=\"none\",\n",
    "    integrator=\"langevin_middle\",\n",
    "    temperature=\"300K\",\n",
    "    qm_engine=engine,\n",
    "    map={\"threads\": 1},\n",
    "    fixed=\"not atoms within 20A of atomidx 3 in mol[1]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360ff3db-bae1-4149-97b8-44c202e119f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the underlying OpenMM context.\n",
    "context = d._d._omm_mols\n",
    "\n",
    "# Get the OpenMM system.\n",
    "omm_system = context.getSystem()\n",
    "\n",
    "# Store a copy of the integrator.\n",
    "integrator = context.getIntegrator().__copy__()\n",
    "\n",
    "# Add the forces to the OpenMM system.\n",
    "omm_system.addForce(restraint_force)\n",
    "\n",
    "# Create a new context.\n",
    "new_context = openmm.Context(omm_system, integrator, context.getPlatform())\n",
    "\n",
    "# Set force constant K for the biasing potential.\n",
    "new_context.setParameter(\"k\", k)\n",
    "\n",
    "#Set center of biasing potential\n",
    "new_context.setParameter(\"r0\", r0)\n",
    "\n",
    "# Set the postions of the new context to be the same as the original context.\n",
    "new_context.setPositions(context.getState(getPositions=True).getPositions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735c0037-262b-483b-b726-ae7176880e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elliot/mambaforge/envs/emle-train-sire/lib/python3.12/site-packages/torch/nn/modules/module.py:1562: UserWarning: at::frobenius_norm is deprecated and it is just left for JIT compatibility. It will be removed in a future PyTorch release. Please use `linalg.vector_norm(A, 2., dim, keepdim)` instead (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1729124132187/work/aten/src/ATen/native/LinearAlgebra.cpp:3072.)\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Sampling production. Trajectories are saved in dcd files.\n",
    "file_handle = open(f\"./AbyU_OpenMM/AbyU_OpenMM.dcd\", \"bw\")\n",
    "state = new_context.getState(getPositions=True)\n",
    "positions = state.getPositions()\n",
    "dcd_file = DCDFile(file_handle, prm.topology, dt=integrator.getStepSize())\n",
    "dcd_file.writeModel(positions)\n",
    "\n",
    "for x in range(100):\n",
    "    integrator.step(10)\n",
    "    state = new_context.getState(getPositions=True)\n",
    "    positions = state.getPositions()\n",
    "    dcd_file.writeModel(positions)\n",
    "        \n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6780038-a686-4c78-91db-5825e641d9e1",
   "metadata": {},
   "source": [
    "# Viewing the trajectory\n",
    "\n",
    "Here we use NGLview to again view the trajectory. Running the cell as is views just the substrate, whilst swapping which trajectory viewing commands are edited out will allow you to view the whole system being simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acf0b95-08f3-4b79-a1f2-692e1695492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5883de462343fea104df40e72a234a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(max_frame=30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traj = sr.load(\"AbyU_OpenMM.prm7\", \"AbyU_OpenMM/AbyU_OpenMM.dcd\")\n",
    "\n",
    "#To view the whole system\n",
    "traj.trajectory().view()\n",
    "\n",
    "# To view just the substrate\n",
    "#traj[1].trajectory().view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmm-workshop",
   "language": "python",
   "name": "mlmm-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
